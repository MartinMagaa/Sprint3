#INTEGRANTES: 
#FABBRONI PABLO TOMÁS
#MAGALLANES MARTÍN

#CARPETA DRIVE CON ARCHIVO DEL SPRINT EN FORMATO .py y .ipynb
#URL: https://drive.google.com/drive/folders/1hj7ZWjQY7znieqNT-6w11vm9sqKwUE2O?usp=sharing

"""SPRINT III

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p6Sisx4FdPqJluTbBKsk2b-ST3a5Z2h-

# 3er entrega - Sistemas de recomendación

En esta entrega vamos a trabajar con un sistema de recomendación.

Vamos a aprovechar la competencia de Telecom que vimos ya que los datos son reales y están buenos para practicar.

En el siguiente repositorio pueden encontrar el significado de cada columna de los datasets:
https://github.com/Datathon2021/Recomendador

Consigna:

- Dividir set en train y test. Tomar como train los datos hasta el 1 de marzo de 2021. Desde el 1ro de marzo en adelante, reservar para test.
- Desarrollar un recomendador. El recomendador debe ser capaz de generar recomendaciones para TODOS los usuarios (incluyendo los cold start que no tengan visualizaciones en el set de train). Generar 20 recomendaciones por usuario.
- Las recomendaciones tienen que ser para cada account_id y hay que recomendar content_id (NO asset_id). Pueden encontrar esto en el repositorio de la competencia.
- Los contenidos que recomienden, no tienen que haber sido vistos previamente por los usuarios (filtrar).
- Evaluarlo con MAP.

Recomendaciones:
- En este caso no tenemos ratings explícitos como los casos que vimos, deben generar ustedes estos ratings mediante algún criterio. Lo más simple podría ser utilizar ratings binarios (lo vió / no lo vió).
- Hay una columna que nos indica hasta cuando va a estar disponible el contenido
- La columna **end_vod_date**: "fecha de finalización de la disponibilidad del activo en la plataforma" puede llegar a serles muy útil. ¿Tiene sentido recomendar algo que no va a estar disponible en el set de test? (a partir del 1 de marzo de 2021).
- Comiencen con algo SIMPLE. No se compliquen con todas las columnas que tiene el dataset. No van a necesitar usar todas, muchas columnas podrán descartarlas dependiendo del enfoque que tomen.

Datos:
- Se encuentran adjuntos en la entrega

Fecha de entrega: 21/08/2024.

Pueden subirlo a un repositorio de github (público) y subir el link. De paso les sirve para ir armando su perfil de github con algunos proyectos 😉.
"""

import pandas as pd

#Cargamos y leemos los datasets.
df_data = pd.read_excel('/content/metadata.xlsx')
df_train = pd.read_csv('/content/train.csv')

df_train = df_train.drop(columns=['customer_id','device_type','tunein','tuneout','resume'])

df_train.head()

df_data.head()

df_movie = df_data[['asset_id','content_id','create_date','end_vod_date']].copy()
df_movie.head()

#Hacemos un Left Join entre df_movie y df_train
df_train1 = df_train.merge(df_movie, how="left", left_on="asset_id", right_on="asset_id")

df_train1.shape

#Creamos una columna nueva = 1 para trabajar posteriormente construir la matriz pivot binaria.
df_train1['Vistas'] = 1

#Cambiamos el formato de las fechas
df_train1["create_date"] = pd.to_datetime(df_train1["create_date"], format='ISO8601')

df_train1

#Quitamos zona horaria y removemos la hora
df_train1["create_date"] = df_train1["create_date"].apply(lambda x: x.replace(tzinfo=None).date())

#Cambiamos el formato de las fechas
df_train1["end_vod_date"] = pd.to_datetime(df_train1["end_vod_date"], format='ISO8601')

#Quitamos zona horaria y removemos la hora
df_train1["end_vod_date"] = df_train1["end_vod_date"].apply(lambda x: x.replace(tzinfo=None).date())

df_train1.head()

#transformamos el tipo de dato a DateTime
df_train1['create_date'] = pd.to_datetime(df_train1['create_date'])

#transformamos el tipo de dato a DateTime
df_train1['end_vod_date'] = pd.to_datetime(df_train1['end_vod_date'])

from datetime import datetime

#Contenido que no estará disponible en el set de test, por ende lo borramos
no_recomms = df_train1[(df_train1.end_vod_date < datetime(year=2021, month=3, day=1))]
no_recomms.shape

df_train1.drop(no_recomms.index, inplace=True)

df_train1.shape

#Dividimos el dataset de train
train = df_train1[df_train1.create_date <= datetime(year=2021, month=3, day=1)]

train.head()

train.shape

#Dividimos el dataset de test
test = df_train1[df_train1.create_date > datetime(year=2021, month=3, day=1)]

test.head()

test.shape

test.account_id.nunique()

train.account_id.nunique()

#2852 cuentas NO están en el train.
test[~test.account_id.isin(train.account_id.unique())].account_id.nunique()

#Creamos la matriz pivote.
train_pivot = pd.pivot_table(train[["account_id", "content_id", "Vistas"]], index='account_id', columns='content_id', values='Vistas')

# Completamos nulos con 0
train_pivot = train_pivot.fillna(0)

train_pivot.head()

#EL SHAPE DE TRAIN ES 109954
train_pivot.shape

#Cramos un diccionario para las futuras referencias.
user_id = list(train_pivot.index)
user_dict = {}
counter = 0
for i in user_id:
    user_dict[i] = counter
    counter += 1

from scipy.sparse import csr_matrix

#Creamos la matrix sparse para comprimir los datos.
train_sparse = csr_matrix(train_pivot.values)

train_sparse

pip install lightfm

from lightfm import LightFM

#Creamos el modelo y lo entrenamos.
model = LightFM(random_state=0,
                loss='warp',
                learning_rate=0.03,
                no_components=20)

model = model.fit(train_sparse,
                  epochs=100,
                  num_threads=16, verbose=False)

"""DataFrame para recomendar los Cold Start"""

#Creamos un dataset dedicado a los usuarios Cold Start
popularity_train = train.groupby("content_id", as_index=False).agg({"account_id":"nunique"}).sort_values(by="account_id", ascending=False)

popularity_train.columns=["content_id", "popularity"]
popularity_train.head(20)

# Transformamos en array el campo 'content_id' para poder pasarlo correctamente por el recomendador
# y evitamos problemas de formato en la salida.
popular_items = list(popularity_train['content_id'])[:20]

"""Desarrollo del recomendador."""

import numpy as np

from tqdm import tqdm

#definimos dict donde vamos a ir almacenando las recomendaciones
recomms_dict = {
    'user_id': [],
    'recomms': [],
    'cold': []
}

#obtenemos cantidad de usuarios y cantidad de items
n_users, n_items = train_pivot.shape
item_ids = np.arange(n_items)

#por cada usuario del dataset de test, generamos recomendaciones
for user in tqdm(test.account_id.unique()):
    #COMPLETAR: Validar si el usuario se encuentra en la matriz de train (train_pivot.index)
    if user in list(train_pivot.index):
      # Si el usuario esta en train, no es cold start. Usamos el modelo para recomendar
      user_x = user_dict[user] #buscamos el indice del usuario en la matriz (transformamos id a indice)

      #COMPLETAR: Generar las predicciones para el usuario x
      preds = model.predict(user_ids=user_x, item_ids = item_ids)

      #COMPLETAR: Basándose en el ejemplo anterior, ordenar las predicciones de menor a mayor y quedarse con 50.
      scores = pd.Series(preds)
      scores.index = train_pivot.columns
      scores = list(pd.Series(scores.sort_values(ascending=False).index))[:20]

      #COMPLETAR: Obtener listado de contenidos vistos anteriormente por el usuario (en el set de train)
      watched_contents = train[train.account_id == user].content_id.unique()

      #COMPLETAR: Filtrar contenidos ya vistos y quedarse con los primeros 10
      recomms = [x for x in scores if x not in watched_contents][:20]

      # Guardamos las recomendaciones en el diccionario
      recomms_dict['user_id'].append(user)
      recomms_dict['recomms'].append(scores)
      recomms_dict['cold'].append(False)

    # En este else trataremos a los usuarios que no están en la matriz (cold start)
    else:
      recomms_dict['user_id'].append(user)
      # Les recomendamos contenido popular
      recomms_dict['recomms'].append(popular_items)
      recomms_dict['cold'].append(True)

recomendaciones = pd.DataFrame(recomms_dict)

recomendaciones.head(20)

recomendaciones.cold.value_counts()

"""Ahora tenemos que comparar nuestras recomms contra lo que los usuarios realmente vieron (test)."""

test.head()

"""Primero ordenamos los activos según los ratings (mayor a menor) de los usuarios en el set de test.

Luego, agrupamos y armamos un listado de los activos para cada usuario.

Este es el listado "ideal" contra el que vamos a comparar nuestras recomendaciones.
"""

ideal_recomms = test.sort_values(by=["account_id", "Vistas"], ascending=False)\
                  .groupby(["account_id"], as_index=False)\
                  .agg({"content_id": "unique"})\
                  .head()
ideal_recomms.head()

df_map = ideal_recomms.merge(recomendaciones, how="left", left_on="account_id", right_on="user_id")[["account_id", "content_id", "recomms"]]
df_map.columns = ["user_id", "ideal", "recomms"]
df_map.head()

aps = []  # lista vacía para ir almacenando la AP de cada recomendación

for pred, label in df_map[["ideal", "recomms"]].values:
    n = len(label)  # cantidad de elementos recomendados
    arange = np.arange(n, dtype=np.int32) + 1.  # indexamos en base 1
    rel_k = np.in1d(label[:n], pred)  # lista de booleanos que indican la relevancia de cada ítem
    tp = np.ones(rel_k.sum(), dtype=np.int32).cumsum()  # lista con el contador de verdaderos positivos
    denom = arange[rel_k]  # posiciones donde se encuentran los ítems relevantes
    if len(denom) > 0:
        ap = (tp / denom).sum() / len(pred)  # average precision
    else:
        ap = 0.0
    print(ap)
    aps.append(ap)

MAP = np.mean(aps)
print(f'mean average precision = {round(MAP, 5)}')

aps = [] # lista vacía para ir almacenando la AP de cada recomendación

for pred, label in df_map[["ideal", "recomms"]].values:
  n = len(pred) # cantidad de elementos recomendados
  arange = np.arange(n, dtype=np.int32) + 1. # indexamos en base 1
  rel_k = np.in1d(pred[:n], label) # lista de booleanos que indican la relevancia de cada ítem
  tp = np.ones(rel_k.sum(), dtype=np.int32).cumsum() # lista con el contador de verdaderos positivos
  denom = arange[rel_k] # posiciones donde se encuentran los ítems relantes
  ap = (tp / denom).sum() / len(label) # average precision
  print(ap)
  aps.append(ap)

MAP = np.mean(aps)
print(f'mean average precision = {round(MAP, 5)}')
